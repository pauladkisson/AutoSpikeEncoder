{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "payCKL-mqhfP"
   },
   "source": [
    "# Benchmark Simulated Data Using SNR cutoff and Holdout Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AGA2oroEWDk1"
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydNiMYYUstbP",
    "outputId": "65da0097-0b91-48ab-cf63-741c3d62f215"
   },
   "outputs": [],
   "source": [
    "## Mount Google Drive Data (If using Google Colaboratory)\n",
    "# try:\n",
    "#     from google.colab import drive\n",
    "#     drive.mount('/content/gdrive')\n",
    "# except:\n",
    "#     print(\"Mounting Failed.\")\n",
    "\n",
    "## External Libraries\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from autoencode import AEEnsemble\n",
    "from datasets import UnsupervisedDataset, SupervisedDataset, BenchmarkDataset\n",
    "from sklearn.mixture import GaussianMixture\n",
    "# !pip install graspologic\n",
    "from graspologic.utils import remap_labels\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2)\n",
    "sns.set_theme()\n",
    "sns.set_context(\"paper\")\n",
    "\n",
    "# General Data Directory ##TODO: Please fill in the appropriate directory\n",
    "# os.chdir(\"/content/\")\n",
    "data_dir = \"/export/gaon1/data/jteneggi/DL/pedreira\"\n",
    "results_dir = \"results\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IUkQnuJ3THva"
   },
   "source": [
    "Define Datasets and AutoEncoder Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q8jI7ndcSiqN",
    "outputId": "21dfc1b5-77dc-40c7-b333-77ce946fe6c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n",
      "Using cuda:0\n",
      "Using cuda:0\n",
      "Using cuda:0\n",
      "Using cuda:0\n",
      "Using cuda:0\n"
     ]
    }
   ],
   "source": [
    "sup_data = BenchmarkDataset(data_dir)\n",
    "test_idx = []\n",
    "for i in range(2, 21):\n",
    "    test_idx.append(list(sup_data.num_units).index(i))\n",
    "train_idx = np.arange(len(sup_data))\n",
    "not_test_mask = np.logical_not(np.isin(train_idx, test_idx))\n",
    "train_idx = train_idx[not_test_mask]\n",
    "train_data = torch.utils.data.Subset(sup_data, train_idx)\n",
    "test_data = torch.utils.data.Subset(sup_data, test_idx)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "ae = AEEnsemble(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6phQWnjUTMV6"
   },
   "source": [
    "Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WRLcWaCmTLgx"
   },
   "outputs": [],
   "source": [
    "def gt_gmm(latent_vecs, test_targets):\n",
    "    test_acc = []\n",
    "    test_prec = []\n",
    "    test_recall = []\n",
    "    session_weights = []\n",
    "    remapped_preds = []\n",
    "    for (i, latent), targets in zip(enumerate(latent_vecs), test_targets):\n",
    "        session_weights.append(len(targets))\n",
    "        units = set(targets)\n",
    "        num_units = len(units)\n",
    "        gmm = GaussianMixture(n_components=num_units)\n",
    "        print(f\"Initialized {num_units}\")\n",
    "        pred = gmm.fit_predict(latent)\n",
    "        print(f\"Fitted\")\n",
    "        remapped_pred = remap_labels(targets, pred)\n",
    "        print(\"Remapped\")\n",
    "        remapped_preds.append(remapped_pred)\n",
    "        prec = []\n",
    "        recall = []\n",
    "        for unit in units:\n",
    "            TP = np.sum(np.logical_and(remapped_pred==unit, targets==unit))\n",
    "            FP = np.sum(np.logical_and(remapped_pred==unit, targets!=unit))\n",
    "            if FP == 0:\n",
    "                prec.append(1)\n",
    "            else:\n",
    "                prec.append(TP / (TP + FP))\n",
    "            TN = np.sum(np.logical_and(remapped_pred!=unit, targets!=unit))\n",
    "            FN = np.sum(np.logical_and(remapped_pred!=unit, targets==unit))\n",
    "            if FN == 0:\n",
    "                recall.append(1)\n",
    "            else:\n",
    "                recall.append(TP / (TP + FN))\n",
    "\n",
    "        test_prec.append(np.mean(prec))\n",
    "        test_recall.append(np.mean(recall))\n",
    "        test_acc.append(sum(remapped_pred==targets)/len(targets))\n",
    "\n",
    "    session_weights = np.array(session_weights) / sum(session_weights)\n",
    "    avg_acc = np.sum(test_acc*session_weights)\n",
    "    avg_prec = np.mean(test_acc)\n",
    "    avg_recall = np.mean(test_recall)\n",
    "    avg_stats = avg_acc, avg_prec, avg_recall\n",
    "    session_stats = test_acc, test_prec, test_recall\n",
    "    return avg_stats, session_stats, remapped_preds\n",
    "\n",
    "def auto_gmm(latent_vecs, test_targets):\n",
    "    test_acc = []\n",
    "    test_prec = []\n",
    "    test_recall = []\n",
    "    session_weights = []\n",
    "    remapped_preds = []\n",
    "    max_n_comps = 21\n",
    "    for (i, latent), targets in zip(enumerate(latent_vecs), test_targets):\n",
    "        session_weights.append(len(targets))\n",
    "        units = set(targets)\n",
    "        num_units = len(units)\n",
    "        bics = []\n",
    "        preds = []\n",
    "        for n_comps in range(1, max_n_comps+1):\n",
    "            gmm = GaussianMixture(n_components=n_comps)\n",
    "            preds.append(gmm.fit_predict(latent))\n",
    "            bics.append(gmm.bic(latent))\n",
    "        pred = preds[np.argmin(bics)]\n",
    "        print(\"predicted num_units=\", np.argmin(bics)+1)\n",
    "        print(\"true num_units=\", num_units)\n",
    "        remapped_pred = remap_labels(targets, pred)\n",
    "        remapped_preds.append(remapped_pred)\n",
    "        prec = []\n",
    "        recall = []\n",
    "        for unit in units:\n",
    "            TP = np.sum(np.logical_and(remapped_pred==unit, targets==unit))\n",
    "            FP = np.sum(np.logical_and(remapped_pred==unit, targets!=unit))\n",
    "            if FP == 0:\n",
    "                prec.append(1)\n",
    "            else:\n",
    "                prec.append(TP / (TP + FP))\n",
    "            TN = np.sum(np.logical_and(remapped_pred!=unit, targets!=unit))\n",
    "            FN = np.sum(np.logical_and(remapped_pred!=unit, targets==unit))\n",
    "            if FN == 0:\n",
    "                recall.append(1)\n",
    "            else:\n",
    "                recall.append(TP / (TP + FN))\n",
    "\n",
    "        test_prec.append(np.mean(prec))\n",
    "        test_recall.append(np.mean(recall))\n",
    "        test_acc.append(sum(remapped_pred==targets)/len(targets))\n",
    "\n",
    "    session_weights = np.array(session_weights) / sum(session_weights)\n",
    "    avg_acc = np.sum(test_acc*session_weights)\n",
    "    avg_prec = np.mean(test_acc)\n",
    "    avg_recall = np.mean(test_recall)\n",
    "    avg_stats = avg_acc, avg_prec, avg_recall\n",
    "    session_stats = test_acc, test_prec, test_recall\n",
    "    return avg_stats, session_stats, remapped_preds\n",
    "\n",
    "def viz_stats(avg_stats, session_stats, _title, figname):\n",
    "    avg_acc, avg_prec, avg_recall = avg_stats\n",
    "    test_acc, test_prec, test_recall = session_stats\n",
    "    print(\"Average Accuracy=\", avg_acc)\n",
    "    print(\"Average Precision=\", avg_prec)\n",
    "    print(\"Average Recall=\", avg_recall)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.hold = True\n",
    "    plt.plot(np.arange(2, 21), test_acc, label=\"Accuracy\")\n",
    "    plt.plot(np.arange(2, 21), test_prec, label=\"Precision\")\n",
    "    plt.plot(np.arange(2, 21), test_recall, label=\"Recall\")\n",
    "    plt.hold = False\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"True Number of Units (before SNR)\")\n",
    "    plt.ylabel(\"Performance\")\n",
    "    plt.title(_title)\n",
    "    plt.savefig(results_dir+\"/\"+figname)\n",
    "\n",
    "def viz_tsne(unit_nums, latent_vecs, test_targets, remapped_preds, fignames):\n",
    "    for idx, figname in zip(unit_nums, fignames):\n",
    "        i = idx - 2 #accounting for offset since idx=0 has 2 units\n",
    "        latent = latent_vecs[i]\n",
    "        targets = test_targets[i]\n",
    "        remapped_pred = remapped_preds[i]\n",
    "\n",
    "        # Plot tsne\n",
    "        latent_manifold = tsne.fit_transform(latent.cpu())\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
    "        ax1.set_title(\"Ground Truth Labels\")\n",
    "        ax1.set_xticks([])\n",
    "        ax1.set_yticks([])\n",
    "        for c in range(np.max(targets)+1):\n",
    "            c_manifold = latent_manifold[targets == c]\n",
    "            ax1.scatter(c_manifold[:, 0], c_manifold[:, 1], marker=\".\", s=.5)\n",
    "\n",
    "        ax2.set_title(\"Predicted Labels\")\n",
    "        for c in range(np.max(remapped_pred)+1):\n",
    "            c_manifold = latent_manifold[remapped_pred == c]\n",
    "            ax2.scatter(c_manifold[:, 0], c_manifold[:, 1], marker=\".\", s=.5)\n",
    "        plt.savefig(results_dir+\"/\"+figname)\n",
    "\n",
    "def embed_test(test_data, ae):\n",
    "    latent_vecs = []\n",
    "    test_targets = []\n",
    "    for spikes, targets, snrs, num_units in test_data:\n",
    "        session_latent = []\n",
    "        possible_targets = np.arange(len(snrs))\n",
    "        hi_fidel_targets = possible_targets[snrs>=min_snr]\n",
    "        spikes = torch.FloatTensor(spikes[np.isin(targets, hi_fidel_targets)])\n",
    "        session_targets = targets[np.isin(targets, hi_fidel_targets)]\n",
    "        if \"cuda\" in ae.device:\n",
    "            spikes = spikes.cuda(0)\n",
    "        for encoder in ae.encoders:\n",
    "            session_latent.append(encoder(spikes))\n",
    "        session_latent = torch.cat(session_latent, dim=1).detach().cpu()\n",
    "        latent_vecs.append(session_latent)\n",
    "        test_targets.append(session_targets)\n",
    "    return latent_vecs, test_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WdBf7zTfV7tX"
   },
   "source": [
    "Run Embeddings for different minimum SNR values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "id": "KxSUmTLkV61w",
    "outputId": "25ae70df-e865-43a7-f190-c6276e2c018b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0\n",
      "Using cuda:0\n",
      "Using cuda:0\n",
      "Using cuda:0\n",
      "Using cuda:0\n",
      "Using cuda:0\n",
      "min_snr= 0\n",
      "min_snr= 8\n",
      "min_snr= 16\n",
      "min_snr= 24\n",
      "min_snr= 0\n",
      "Embedded\n",
      "Initialized 3\n",
      "Fitted\n",
      "Remapped\n",
      "Initialized 4\n",
      "Fitted\n",
      "Remapped\n",
      "Initialized 5\n",
      "Fitted\n",
      "Remapped\n",
      "Initialized 6\n",
      "Fitted\n",
      "Remapped\n",
      "Initialized 7\n",
      "Fitted\n",
      "Remapped\n",
      "Initialized 8\n",
      "Fitted\n",
      "Remapped\n",
      "Initialized 9\n",
      "Fitted\n",
      "Remapped\n",
      "Initialized 10\n",
      "Fitted\n",
      "Remapped\n",
      "Initialized 11\n",
      "Fitted\n",
      "Remapped\n",
      "Initialized 12\n",
      "Fitted\n",
      "Remapped\n",
      "Initialized 13\n",
      "Fitted\n",
      "Remapped\n",
      "Initialized 14\n",
      "Fitted\n",
      "Remapped\n",
      "Initialized 15\n",
      "Fitted\n",
      "Remapped\n",
      "Initialized 16\n",
      "Fitted\n",
      "Remapped\n",
      "Initialized 17\n",
      "Fitted\n",
      "Remapped\n",
      "Initialized 18\n",
      "Fitted\n",
      "Remapped\n",
      "Initialized 19\n",
      "Fitted\n",
      "Remapped\n",
      "Initialized 20\n",
      "Fitted\n",
      "Remapped\n",
      "Initialized 21\n",
      "Fitted\n",
      "Remapped\n",
      "GMM\n",
      "Average Accuracy= 0.7995255565056685\n",
      "Average Precision= 0.7994866154407538\n",
      "Average Recall= 0.7990845592277145\n",
      "t-SNE\n",
      "predicted num_units= 7\n",
      "true num_units= 3\n"
     ]
    }
   ],
   "source": [
    "min_snrs = [0, 8, 16, 24]\n",
    "ae = AEEnsemble(convolutional_encoding=True, device=device)\n",
    "ae_is_trained = [True, True, True, True] #whether the autoencoder is already trained on the corresponding snr\n",
    "#Train Loop\n",
    "for min_snr, is_trained in zip(min_snrs, ae_is_trained):\n",
    "    print(\"min_snr=\", min_snr)\n",
    "    if is_trained:\n",
    "        continue\n",
    "    ae.benchmark(min_snr, train_data, test_data, on_drive=True)\n",
    "\n",
    "#Eval Loop\n",
    "snr_acc, snr_prec, snr_recall = [], [], []\n",
    "for min_snr, is_trained in zip(min_snrs, ae_is_trained):\n",
    "    print(\"min_snr=\", min_snr)\n",
    "    prefix=\"benchmark_snr_%s\"%min_snr\n",
    "    ae.load()\n",
    "    latent_vecs, test_targets = embed_test(test_data, ae)\n",
    "    print(\"Embedded\")\n",
    "    avg_stats, session_stats, remapped_preds = gt_gmm(latent_vecs, test_targets)\n",
    "    print(\"GMM\")\n",
    "    acc, prec, recall = avg_stats\n",
    "    accs, precs, recalls = [], [], []\n",
    "    accs.append(acc)\n",
    "    precs.append(prec)\n",
    "    recalls.append(recall)\n",
    "    viz_stats(avg_stats, session_stats, \"Ground-Truth GMM: minimum SNR=%s\"%min_snr, \"gtgmm_stats_snr%s\"%min_snr)\n",
    "    fignames = [\"gtgmm_tsne_snr%s_numunits%s\"%(min_snr, n) for n in [2, 10, 20]]\n",
    "    viz_tsne([2, 10, 20], latent_vecs, test_targets, remapped_preds, fignames)\n",
    "    print(\"t-SNE\")\n",
    "    avg_stats, session_stats, remapped_preds = auto_gmm(latent_vecs, test_targets)\n",
    "    print(\"auto-GMM\")\n",
    "    acc, prec, recall = avg_stats\n",
    "    accs.append(acc)\n",
    "    precs.append(prec)\n",
    "    recalls.append(recall)\n",
    "    viz_stats(avg_stats, session_stats, \"Auto GMM: minimum SNR=%s\"%min_snr, \"autogmm_stats_snr%s\"%min_snr)\n",
    "    fignames = [\"autogmm_tsne_snr%s_numunits%s\"%(min_snr, n) for n in [2, 10, 20]]\n",
    "    viz_tsne([2, 10, 20], latent_vecs, test_targets, remapped_preds, fignames)\n",
    "\n",
    "    snr_acc.append(accs)\n",
    "    snr_prec.append(precs)\n",
    "    snr_recall.append(recalls)\n",
    "\n",
    "snr_acc = np.array(snr_acc)\n",
    "np.save(results_dir+\"/snr_acc.npy\", snr_acc)\n",
    "snr_prec = np.array(snr_prec)\n",
    "np.save(results_dir+\"/snr_prec.npy\", snr_prec)\n",
    "snr_recall = np.array(snr_recall)\n",
    "np.save(results_dir+\"/snr_recall.npy\", snr_recall)\n",
    "plt.figure()\n",
    "plt.hold = True\n",
    "plt.plot(min_snrs, snr_acc[:, 0], label=\"Average Accuracy\", c=\"C0\", ls=\"-\")\n",
    "plt.plot(min_snrs, snr_acc[:, 1], c=\"C0\", ls=\"--\")\n",
    "plt.plot(min_snrs, snr_prec[:, 0], label=\"Average Precision\", c=\"C1\", ls=\"-\")\n",
    "plt.plot(min_snrs, snr_prec[:, 1], c=\"C1\", ls=\"--\")\n",
    "plt.plot(min_snrs, snr_recall[:, 0], label=\"Average Recall\", c=\"C2\", ls=\"-\")\n",
    "plt.plot(min_snrs, snr_recall[:, 1], c=\"C2\", ls=\"--\")\n",
    "plt.hold = False\n",
    "plt.xlabel(\"Minimum SNR\")\n",
    "plt.ylabel(\"Performance\")\n",
    "plt.title(\"V0\")\n",
    "plt.savefig(results_dir+\"/v0_snr_stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BtUIlNCcEtOa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BenchmarkSim.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
